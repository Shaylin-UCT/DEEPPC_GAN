MoCo v3. 
We employ a contrastive-based self-supervised
classification framework in the form of MoCo v3 - a simpler, more
accurate and stable version of MoCo [15] as proposed by Chen et
al [5]. Version 3 removes the memory queue used by MoCo and
implements a symmetrized loss function. The model follows that of
Chen et al. [5] with a ResNet-50 backbone, LARS optimizer, batch
size of 4096, a learning rate of 0.3, a weight decay of 1.5e-6 and the
temperature parameter set to 1. The momentum coefficient used to
train the key autoencoder is set to 0.996

Vanilla: 
    Adam hyperparameters comes from Hu et al., ‘Unsupervised Learning for Cell-Level Visual Representation in Histopathology Images with Generative Adversarial Networks’.


WGAN-GP (all from "Improved training of wasserstein GAN")
    Generator
        Adam - α = 0.0001, β1 = 0, β2 = 0.9
        λ=10
        n_crit = 10
    Discriminator
        "To resolve this, we simply omit batch normalization in the critic in our models, finding that they perform well without it." -> improved training of WGAN-GP
    Overall
        Adam learning rate = 0.0001 (can be 0.00005 like in "Cancer diagnosis using GAN...imbalanced data")

    Made channel default = 3 and batch size = 64. Everything else was kept constant
StyleGAN
Moco v3:

    hyperparameters [based on "An empirical study of training self-supervised ViT]
    ResNet-50
    Batch = 4096
    LARS Optimizer
    lr = 0.3
    wd = 1.5e-6
    temp = 1.0
    Fk momenum coefficient = 0.996 but increases to 1 with cosine schedule
    # Running MoCo v3 
    ## ResNet-50, 300-epoch training:
        Needs 2 nodes, On the first use:

        python main_moco.py \
        --lr=.3 --epochs=300 \
        --moco-m-cos --crop-min=.2 \
        --dist-url 'tcp://[your first node address]:[specified port]' \
        --multiprocessing-distributed --world-size 2 --rank 0 \
        [your imagenet-folder with train and val folders]

        On the second node, do the same but with "--rank 1" added in 

    ## ResNet-50, Linear Classification
        python main_lincls.py \
    --dist-url 'tcp://localhost:10001' \
    --multiprocessing-distributed --world-size 1 --rank 0 \
    --pretrained [your checkpoint path]/[your checkpoint file].pth.tar \
    [your imagenet-folder with train and val folders]

    ## Pretrained (on ImageNet) is available on Git (https://github.com/facebookresearch/moco-v3/blob/main/CONFIG.md) but
    ## This won't work on medical datasets -> need pretraining on medical database
Dataloader:
    for i, (imgs, _) in enumerate(dataloader):
    i -> batch number
    imgs -> actual image
    _ -> the label of the image
